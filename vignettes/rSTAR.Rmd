---
title: "Getting Started with rSTAR"
output: rmarkdown::html_vignette
number_sections: true
vignette: >
  %\VignetteIndexEntry{Getting Started with rSTAR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

bibliography: refs.bib
link-citations: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
## Introduction
The `rSTAR` package implements a variety of methods to analyze diverse integer-valued data, with a special focus on count-valued data. The package functionality is broadly split into three categories: Bayesian estimation of STAR models (@Kowal_Canale_STAR, @Kowal_Wu_Conjugate), frequentist/classical estimation (@Kowal_Wu_Frequentist), and time series analysis using warped Dynamic Linear Models (@king2021warped).

We give a brief description of the STAR framework, before diving into specific examples that show the `rSTAR` functionality.

## Simultaneous Transformation and Rounding (STAR) Models

STAR models build upon continuous data models to provide a *valid integer-valued data-generating process*. An example STAR model for linear regression is as follows:
\begin{align*}
y_i &= \mbox{floor}(y_i^*) \\
z_i^* &= \log(y_i^*) \\
z_i^* &= x_i'\beta + \epsilon_i, \quad \epsilon_i \stackrel{iid}{\sim}N(0, \sigma^2)
\end{align*}
The latent data $y_i^*$ act as a *continuous proxy* for the count data $y_i$, which is easier to model yet has a simple mapping via the floor function to the observed data. The latent data $y_i^*$ are transformed to $z_i^*$, as in common practice, and modeled using Gaussian linear regression. This model inherits the same structure as before, but the data-generating process is now integer-valued.  

More generally, STAR models are defined via a *rounding operator* $h$, a (known or unknown) *transformation* $g$, and a *continuous data model* $\Pi_\theta$ with unknown parameters $\theta$:
\begin{align*}
y &= h(y^*) \quad \mbox{(rounding)}\\
z^* &= g(y^*) \quad \mbox{(transformation)}\\
z^* & \sim \Pi_\theta \quad \mbox{(model)}\\
\end{align*}
Importantly, STAR models are highly flexible integer-valued processes, and provide the capability to model (i) discrete data, (ii) zero-inflation, (iii) over- or under-dispersion, and (iv) bounded or censored data. 

We focus on conditionally Gaussian models of the form
$$
z^*(x) = \mu_\theta(x) + \epsilon(x), \quad \epsilon(x) \stackrel{iid}{\sim}N(0, \sigma^2)
$$
where $\mu_\theta(x)$ is the conditional expectation of the transformed latent data with unknown parameters $\theta$. Examples include linear, additive, and tree-based regression models. 

## Integer-Valued Data: The Roaches Dataset
As an illustration, consider the `roaches` data from @Gelman_Hill_2006.
The response variable, $y_i$, is the number of roaches caught in traps in apartment $i$, with $i=1,\ldots, n = 262$. 

```{r roaches}
# Source: http://mc-stan.org/rstanarm/articles/count.html
#install.packages("rstanarm")
data(roaches, package="rstanarm") 

# Roaches:
y = roaches$y

# Function to plot the point mass function:
stickplot = function(y, ...){
  js = 0:max(y); 
  plot(js, 
       sapply(js, function(js) mean(js == y)), 
       type='h', lwd=2, ...)
}
stickplot(y, main = 'PMF: Roaches Data',
          xlab = 'Roaches', ylab = 'Probability mass')
```

There are several notable features in the data:

1. Zero-inflation: `r round(100*mean(y==0), 0)`\% of the observations are zeros. 
2. (Right-) Skewness, which is clear from the histogram and common for (zero-inflated) count data.
3. Overdispersion: the sample mean is `r round(mean(y),0)` and the sample variance is `r round(var(y),0)`. 

A pest management treatment was applied to a subset of 158 apartments, with the remaining 104 apartments receiving a control. Additional data are available on the pre-treatment number of roaches, whether the apartment building is restricted to elderly residents, and the number of days for which the traps were exposed. We are interested in modeling how the roach incidence varies with these predictors.

## Frequentist inference for STAR models
Frequentist (or classical) estimation and inference for STAR models is provided by an EM algorithm. Sufficient for estimation is an `estimator` function which solves the least squares (or Gaussian maximum likelihood) problem associated with $\mu_\theta$---or in other words, the estimator that *would* be used for Gaussian or continuous data. Specifically, `estimator` inputs data and outputs a list with two elements: the estimated `coefficients` $\hat \theta$ and the `fitted.values` $\hat \mu_\theta(x_i) = \mu_{\hat \theta}(x_i)$. The EM algorithm updates the parameters until convergence to the maximum likelihood estimators (MLEs), and is implemented in `star_EM`:

```{r freq-lm}
library(rSTAR)

# Select a transformation:
transformation = 'log' # Log transformation
# transformation = 'np' # Estimated transformation using empirical CDF

# EM algorithm for STAR (using the log-link)
fit_em = lm_star(y ~ roach1 + treatment + senior + log(exposure2),
                 data = roaches, transformation = transformation)


# Dimensions:
n = nrow(fit_em$X); p = ncol(fit_em$X)

# Fitted coefficients:
round(coef(fit_em), 3)
```

Here the `log` transformation was used, but other options are available; see `?lm_star` for details. 

Based on the fitted STAR model, we may further obtain *confidence intervals* for the estimated coefficients using `confint`:

```{r conf}
# Confidence interval for the j=2 column:
j = 2
ci_j = confint(fit_em, level = 0.95,
        j = j,
        include_plot = FALSE)
print(round(ci_j, 3))

# Confidence for all columns:
ci_all = sapply(1:p, function(j)
  confint(fit_em, level = 0.95,
        j = j,
        include_plot = FALSE))
colnames(ci_all) = colnames(fit_em$X); 
rownames(ci_all) = c('Lower', 'Upper')
print(t(round(ci_all, 3)))

```

Similarly, *p-values* are available using likelihood ratio tests, which can be applied for individual coefficients,

$$
H_0: \beta_j= 0 \quad \mbox{vs} \quad H_1: \beta_j \ne 0
$$

or for joint sets of variables, analogous to a (partial) F-test:

$$
H_0: \beta_1=\ldots=\beta_p = 0, \quad \mbox{vs.} \quad H_1: \beta_j \ne 0 \mbox{ for some } j=1,\ldots,p
$$
```{r pval-all}
# p-value for *any* effects
# Note that the null model estimator *does not* include any X:
# P-values:
print(pvals(fit_em))
```

## Bayesian inference for STAR models

For a Bayesian model, STAR requires only an algorithm for *initializing and sampling* from the posterior distribution under a *continuous data model*. More specifically, posterior inference under STAR is based on a Gibbs sampler, which augments the aforementioned continuous sampler with a draw from $[z^* | y, \theta]$. When $\Pi_\theta$ is conditionally Gaussian, $[z^* | y, \theta]$ is a truncated Gaussian distribution. 

As an illustration, consider the Bayesian linear regression model
\begin{align*}
z_i^* &= x_i'\beta + \epsilon_i, \quad \epsilon_i \stackrel{iid}{\sim}N(0, \sigma^2)\\
\beta_j & \stackrel{iid}{\sim}N(0, \sigma_\beta^2), \quad \sigma_\beta \sim \mbox{Uniform}(0, 10^4)
\end{align*}
With STAR and a log transformation, posterior samples for $(\beta, \sigma_\beta, \sigma)$ are obtained using `star_MCMC` as follows:

```{r bayes-lm}
X = model.matrix(y ~ roach1 + treatment + senior + log(exposure2),
                 data = roaches)

# Dimensions:
n = nrow(X); p = ncol(X)

fit_mcmc = genMCMC_star(y = y,
                          sample_params = function(y, params)
                            sample_params_lm(y, X, params),
                         init_params = function(y) 
                           init_params_lm(y,X),
                         transformation = 'log', verbose = FALSE)
```

The function `sample_params` computes a single draw of the parameters `params` conditional on continuous data. Here, the update is for Bayesian linear Gaussian regression, which samples from the posterior of $(\beta, \sigma_\beta, \sigma)$ conditional on the continuous latent data $z^*$. The function `init_params` simply initializes the parameters `params`. Posterior expectations and posterior credible intervals are available as follows:

```{r estimates-bayes}
# Posterior mean of each coefficient:
round(coef(fit_mcmc),3)

# Credible intervals for regression coefficients
ci_all_bayes = apply(fit_mcmc$post.coefficients[,1:p],
      2, function(x) quantile(x, c(.025, .975)))

# Rename and print:
colnames(ci_all_bayes) = colnames(X); rownames(ci_all_bayes) = c('Lower', 'Upper')
print(t(round(ci_all_bayes, 3)))
```

We may further evaluate the model based on posterior diagnostics and posterior predictive checks on the simulated versus observed proportion of zeros:

```{r diag}
# Posterior draws of the regression coefficients:
post.coef = fit_mcmc$post.coefficients[,2:p]
colnames(post.coef) = colnames(X)[2:p]

# MCMC diagnostics:
plot(as.ts(post.coef), main = 'Trace plots', cex.lab = .75)

# (Summary of) effective sample sizes across coefficients:
getEffSize(post.coef)

# Posterior predictive check:
hist(apply(fit_mcmc$post.pred, 1,
           function(x) mean(x==0)), main = 'Proportion of Zeros', xlab='');
abline(v = mean(y==0), lwd=4, col ='blue')
```


## Additional features in `rSTAR` 

* A fixed upper bound, `y_max`, with $y(x) \le \mbox{}$ `y_max` for all $x$ 

* Residual diagnostics in `star_EM`

* Log-likelihood at MLEs in `star_EM` for model comparison and information criteria (AIC or BIC)

* Customized functions for STAR with gradient boosting `gbm_star` and STAR with random forests `randomForest_star`

* Fitted values $\hat y(x) = E\{y(x)\}$ and posterior samples from $[\hat y(x) | y]$ in `star_MCMC`

* Samples from the *integer-valued* posterior predictive distribution $[\tilde y(x) | y]$ in `star_MCMC`, where $\tilde y$ denotes future or unobserved data

* Customized samplers for Bayesian additive models `sample_params_additive` and linear models with horseshoe priors `sample_params_lm_hs` 

* WAIC and pointwise log-likelihoods for (Bayesian) model comparisons

* A customized STAR model for Bayesian additive regression trees (BART) `bart_star_MCMC`

* Posterior samplers for STAR with unknown and nonparametric transformation $g$

* Monte Carlo samplers for posterior and predictive inference with linear regression and spline regression
