---
title: "Getting Started with countSTAR"
output: rmarkdown::html_vignette
number_sections: true
vignette: >
  %\VignetteIndexEntry{Getting Started with countSTAR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

bibliography: refs.bib
link-citations: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
# Introduction
The `countSTAR` package implements a variety of methods to analyze diverse integer-valued data, with a special focus on count-valued data. The package functionality is broadly split into three categories: Bayesian estimation of STAR models (@Kowal_Canale_STAR, @Kowal_Wu_Conjugate), frequentist/classical estimation (@Kowal_Wu_Frequentist), and time series analysis using warped Dynamic Linear Models (@king2021warped).

We give a brief description of the STAR framework, before diving into specific examples that show the `countSTAR` functionality.

# Simultaneous Transformation and Rounding (STAR) Models

STAR models build upon continuous data models to provide a *valid integer-valued data-generating process*. An example STAR model for linear regression is as follows:
\begin{align*}
y_i &= \mbox{floor}(y_i^*) \\
z_i^* &= \log(y_i^*) \\
z_i^* &= x_i'\beta + \epsilon_i, \quad \epsilon_i \stackrel{iid}{\sim}N(0, \sigma^2)
\end{align*}
The latent data $y_i^*$ act as a *continuous proxy* for the count data $y_i$, which is easier to model yet has a simple mapping via the floor function to the observed data. The latent data $y_i^*$ are transformed to $z_i^*$, as in common practice, and modeled using Gaussian linear regression. This model inherits the same structure as before, but the data-generating process is now integer-valued.  

More generally, STAR models are defined via a *rounding operator* $h$, a (known or unknown) *transformation* $g$, and a *continuous data model* $\Pi_\theta$ with unknown parameters $\theta$:
\begin{align*}
y &= h(y^*) \quad \mbox{(rounding)}\\
z^* &= g(y^*) \quad \mbox{(transformation)}\\
z^* & \sim \Pi_\theta \quad \mbox{(model)}\\
\end{align*}
Importantly, STAR models are highly flexible integer-valued processes, and provide the capability to model (i) discrete data, (ii) zero-inflation, (iii) over- or under-dispersion, and (iv) bounded or censored data. 

We focus on conditionally Gaussian models of the form
$$
z^*(x) = \mu_\theta(x) + \epsilon(x), \quad \epsilon(x) \stackrel{iid}{\sim}N(0, \sigma^2)
$$
where $\mu_\theta(x)$ is the conditional expectation of the transformed latent data with unknown parameters $\theta$. Examples include linear, additive, and tree-based regression models. 

## The Rounding Operator
The rounding operator $h$ is a many-to-one function that sets $y = j$ whenever $y^*\in \mathcal{A}_j$ or equivalently when $z^*=g(y^*) \in g(\mathcal{A}_j)$ .  It could take many different forms, but generally the floor function, i.e  where $\mathcal{A}_j := [j, j+1)$ works well as a default, with the modification $g(\mathcal{A}_0) := (-\infty, 0)$ so that $y = 0$ whenever $z^* < 0$. This latter modification ensures that much of the latent space is mapped to zero, and therefore STAR models can easily account for zero-inflation. Furthermore, when there is a known upper bound `y_max`$=K$ for the data, there is an additional change to incorporate this structure, namely we let $g(\mathcal{A}_K) := [g(a_K), \infty)$. 

The rounding operator and its inverse are implemented in `countSTAR` with the functions `a_j()` and `round_fun()`, although these rarely need to be employed by the end user. Instead, the only thing that might need to be specified by the user would be whether an upper bound for the data exists, in which case there is a simple option of setting `y_max` in each of the modeling functions that will then be passed to the rounding function. 

## The Transformation Function
There are a variety of options for the transformation function $g$, ranging from fixed functions to a-priori data-driven transformations to transformations learned along with the rest of the model. All models in `countSTAR` support three common fixed transformations: log, square root ('sqrt'), and the identity transformation (essentially a rounding-only model). Furthermore, all functions support a set of transformations which are learned by matching marginal moments of the data $y$ to the latent $z$. In particular, "transformation='pois'" uses a moment-matched marginal Poisson CDF, "transformation='neg-bin'" uses a moment-matched marginal Negative Binomial CDF, and finally "transformation='np'" is a nonparametric transformation estimated from the empirical CDF of $y$. Details on the estimation of these transformations can be found in @Kowal_Wu_Frequentist.  In particular the "np" transformation has proven very effective, especially for heaped data, and is the default across all functions.

Some STAR methods also support ways of learning the transformation alongside the model. The "box-cox" transformation assumes the transformation falls in the Box-Cox family and the sampler samples the $\lambda$ parameter. With "transformation='ispline'", the transformation is modeled as an unknown, monotone function using I-splines. The Robust Adaptive Metropolis (RAM) sampler
is used for drawing the parameter of the transformation function. Finally, the "bnp" option models the transformation using the Bayesian bootstrap, which is a Bayesian nonparametric model and incorporates the uncertainty about the transformation into posterior and predictive inference. These learned transformations are not always available in every function; check the appropriate help page to see what options are supported.


# Integer-Valued Data: The Roaches Dataset
As an example of complex count-valued data, consider the `roaches` data from @Gelman_Hill_2006.
The response variable, $y_i$, is the number of roaches caught in traps in apartment $i$, with $i=1,\ldots, n = 262$. 

```{r roaches}
# Source: http://mc-stan.org/rstanarm/articles/count.html
#install.packages("rstanarm")
data(roaches, package="rstanarm") 

# Roaches:
y = roaches$y

# Function to plot the point mass function:
stickplot = function(y, ...){
  js = 0:max(y); 
  plot(js, 
       sapply(js, function(js) mean(js == y)), 
       type='h', lwd=2, ...)
}
stickplot(y, main = 'PMF: Roaches Data',
          xlab = 'Roaches', ylab = 'Probability mass')
```

There are several notable features in the data:

1. Zero-inflation: `r round(100*mean(y==0), 0)`\% of the observations are zeros. 
2. (Right-) Skewness, which is clear from the histogram and common for (zero-inflated) count data.
3. Overdispersion: the sample mean is `r round(mean(y),0)` and the sample variance is `r round(var(y),0)`. 

A pest management treatment was applied to a subset of 158 apartments, with the remaining 104 apartments receiving a control. Additional data are available on the pre-treatment number of roaches, whether the apartment building is restricted to elderly residents, and the number of days for which the traps were exposed. We are interested in modeling how the roach incidence varies with these predictors.

# Frequentist inference for STAR models
Frequentist (or classical) estimation and inference for STAR models is provided by an EM algorithm. Sufficient for estimation is an `estimator` function which solves the least squares (or Gaussian maximum likelihood) problem associated with $\mu_\theta$---or in other words, the estimator that *would* be used for Gaussian or continuous data. Specifically, `estimator` inputs data and outputs a list with two elements: the estimated `coefficients` $\hat \theta$ and the `fitted.values` $\hat \mu_\theta(x_i) = \mu_{\hat \theta}(x_i)$.

## The Classical Linear Model
For many applications, the STAR linear model is often the first method to try. In `countSTAR`, the linear model is implemented with the `lm_star` function, which aims to mimic the functionality of `lm` by allowing users to input a formula. Standard functions like `coef` and `fitted` can be used on the output to extract coefficients and fitted values, respectively.

```{r freq-lm}
library(countSTAR)

# Select a transformation:
transformation = 'log' # Log transformation
#transformation = 'np' # Estimated transformation using empirical CDF

# EM algorithm for STAR (using the log-link)
fit_em = lm_star(y ~ roach1 + treatment + senior + log(exposure2),
                 data = roaches, transformation = transformation)


# Dimensions:
n = nrow(fit_em$X); p = ncol(fit_em$X)

# Fitted coefficients:
round(coef(fit_em), 3)
```

Here the `log` transformation was used, but other options are available; see `?lm_star` for details. 

Based on the fitted STAR linear model, we may further obtain *confidence intervals* for the estimated coefficients using `confint`:

```{r conf}
# Confidence interval for the j=2 column:
j = 2
ci_j = confint(fit_em, level = 0.95,
        j = j,
        include_plot = FALSE)
print(round(ci_j, 3))

# Confidence for all columns:
ci_all = sapply(1:p, function(j)
  confint(fit_em, level = 0.95,
        j = j,
        include_plot = FALSE))
colnames(ci_all) = colnames(fit_em$X); 
rownames(ci_all) = c('Lower', 'Upper')
print(t(round(ci_all, 3)))

```

Similarly, *p-values* are available using likelihood ratio tests, which can be applied for individual coefficients,

$$
H_0: \beta_j= 0 \quad \mbox{vs} \quad H_1: \beta_j \ne 0
$$

or for joint sets of variables, analogous to a (partial) F-test:

$$
H_0: \beta_1=\ldots=\beta_p = 0, \quad \mbox{vs.} \quad H_1: \beta_j \ne 0 \mbox{ for some } j=1,\ldots,p
$$
P-values for all individual coefficients as well as the p-value for *any* effects are computed with the `pvals` function.
```{r pval}
# P-values:
print(pvals(fit_em))
```

Finally, we can get predictions at new data points (or the training data) using `predict`, which actually outputs samples from the . Optionally, prediction intervals can be estimated using the (plug-in) predictive distribution at the MLEs (see ```?predict.lmstar``` for details). Note that the "plug-in" predictive distribution is a crude approximation, and better approaches for uncertainty quantification are available using the Bayesian models.
```{r predict-lm}
#Compute the predictive draws (just using observed points here)
y_pred = predict(fit_em)
```

## Machine Learning Models
In addition to the linear model, `countSTAR` also has implementations for STAR models paired with more flexible regression methods, in particular random forests (`randomForest_star()`) and generalized boosted machines (`gbm_star()`). In these functions, the user directly inputs the set of predictors $X$ alongside any test points in $X_test$, as can be seen in the example below
```{r freq-ml}
# Select a transformation:
transformation = 'np' # Estimated transformation using empirical CDF

# Construct data matrix
y = roaches$y
X = roaches[, c("roach1", "treatment", "senior", "exposure2")]

#Fit STAR with random forests
fit_rf = randomForest_star(y, X, transformation = transformation)

#Fit STAR with GBM
fit_gbm = gbm_star(y, X, transformation = transformation)
```
For all frequentist models, the functions output log-likelihood values at the MLEs, which allows for a quick comparison of model fit.
```{r freq-modelcomp}
#Look at -2*log-likelihood
print(-2*c(fit_rf$logLik, fit_gbm$logLik))
```
In this case, it seems the GBM model has better fit to the data, although this could be further backed up by performing an out-of-sample comparison of the two models.

# Bayesian inference for STAR models

For a Bayesian model, STAR requires only an algorithm for *initializing and sampling* from the posterior distribution under a *continuous data model*. More specifically, posterior inference under STAR is based on a Gibbs sampler, which augments the aforementioned continuous sampler with a draw from $[z^* | y, \theta]$. When $\Pi_\theta$ is conditionally Gaussian, $[z^* | y, \theta]$ is a truncated Gaussian distribution. 

As an illustration, consider the Bayesian linear regression model
\begin{align*}
z_i^* &= x_i'\beta + \epsilon_i, \quad \epsilon_i \stackrel{iid}{\sim}N(0, \sigma^2)\\
\beta_j & \stackrel{iid}{\sim}N(0, \sigma_\beta^2), \quad \sigma_\beta \sim \mbox{Uniform}(0, 10^4)
\end{align*}
With STAR and a log transformation, posterior samples for $(\beta, \sigma_\beta, \sigma)$ are obtained using `star_MCMC` as follows:

```{r bayes-lm}
X = model.matrix(y ~ roach1 + treatment + senior + log(exposure2),
                 data = roaches)

# Dimensions:
n = nrow(X); p = ncol(X)

fit_mcmc = genMCMC_star(y = y,
                          sample_params = function(y, params)
                            countSTAR:::sample_lm_ridge(y, X, params),
                         init_params = function(y) 
                           countSTAR:::init_lm_ridge(y,X),
                         transformation = 'log', verbose = FALSE)
```

The function `sample_params` computes a single draw of the parameters `params` conditional on continuous data. Here, the update is for Bayesian linear Gaussian regression, which samples from the posterior of $(\beta, \sigma_\beta, \sigma)$ conditional on the continuous latent data $z^*$. The function `init_params` simply initializes the parameters `params`. Posterior expectations and posterior credible intervals are available as follows:

```{r estimates-bayes}
# Posterior mean of each coefficient:
round(coef(fit_mcmc),3)

# Credible intervals for regression coefficients
ci_all_bayes = apply(fit_mcmc$post.beta,
      2, function(x) quantile(x, c(.025, .975)))

# Rename and print:
colnames(ci_all_bayes) = colnames(X); rownames(ci_all_bayes) = c('Lower', 'Upper')
print(t(round(ci_all_bayes, 3)))
```

We may further evaluate the model based on posterior diagnostics and posterior predictive checks on the simulated versus observed proportion of zeros:

```{r diag}
# Posterior draws of the regression coefficients:
post.coef = fit_mcmc$post.beta[,2:p]
colnames(post.coef) = colnames(X)[2:p]

# MCMC diagnostics:
plot(as.ts(post.coef), main = 'Trace plots', cex.lab = .75)

# (Summary of) effective sample sizes across coefficients:
getEffSize(post.coef)

# Posterior predictive check:
hist(apply(fit_mcmc$post.pred, 1,
           function(x) mean(x==0)), main = 'Proportion of Zeros', xlab='');
abline(v = mean(y==0), lwd=4, col ='blue')
```


## Additional features in `countSTAR` 

* Residual diagnostics in `star_EM`

* Fitted values $\hat y(x) = E\{y(x)\}$ and posterior samples from $[\hat y(x) | y]$ in `star_MCMC`

* Customized samplers for Bayesian additive models `sample_params_additive` and linear models with horseshoe priors `sample_params_lm_hs` 

* WAIC and pointwise log-likelihoods for (Bayesian) model comparisons

* Monte Carlo samplers for posterior and predictive inference with linear regression and spline regression
