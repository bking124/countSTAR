---
title: "Getting Started with rSTAR"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with rSTAR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(rSTAR)
```


### Simultaneous Transformation and Rounding (STAR) Models

STAR models build upon the continuous data model to provide a *valid integer-valued data-generating process*. An example STAR model for linear regression is as follows:
\begin{align*}
y_i &= \mbox{floor}(y_i^*) \\
z_i^* &= \log(y_i^*) \\
z_i^* &= x_i'\beta + \epsilon_i, \quad \epsilon_i \stackrel{iid}{\sim}N(0, \sigma^2)
\end{align*}
The latent data $y_i^*$ act as a *continuous proxy* for the count data $y_i$, which is easier to model yet has a simple mapping via the floor function to the observed data. The latent data $y_i^*$ are transformed to $z_i^*$, as in common practice, and modeled using Gaussian linear regression. This model inherits the same structure as before, but the data-generating process is now integer-valued.  

More generally, STAR models are defined via a *rounding operator* $h$, a (known or unknown) *transformation* $g$, and a *continuous data model* $\Pi_\theta$ with unknown parameters $\theta$:
\begin{align*}
y &= h(y^*) \quad \mbox{(rounding)}\\
z^* &= g(y^*) \quad \mbox{(transformation)}\\
z^* & \sim \Pi_\theta \quad \mbox{(model)}\\
\end{align*}
Importantly, STAR models are highly flexible integer-valued processes, and provide the capability to model (i) discrete data, (ii) zero-inflation, (iii) over- or under-dispersion, and (iv) bounded or censored data. 

We focus on conditionally Gaussian models of the form
$$
z^*(x) = \mu_\theta(x) + \epsilon(x), \quad \epsilon(x) \stackrel{iid}{\sim}N(0, \sigma^2)
$$
where $\mu_\theta(x)$ is the conditional expectation of the transformed latent data with unknown parameters $\theta$. Examples include linear, additive, and tree-based regression models. 


Estimation, inference, and prediction for STAR are available for both *Bayesian* and *frequentist* models. Implementation in both cases is provided in the `rSTAR` package. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.path = "man/figures/")
library(rstanarm)
library(rSTAR)
```

## Background: integer-valued data
Integer-valued or count data are common in many fields. Frequently, integer-valued data are observed jointly with predictors, over time intervals, or across spatial locations. Integer-valued data also exhibit a variety of distributional features, including zero-inflation, skewness, over- and underdispersion, and in some cases may be bounded or censored. Flexible and interpretable models for *integer-valued processes* are therefore highly useful in practice. 


As an illustration, consider the `roaches` data from Gelman and Hill (2006).
The response variable, $y_i$, is the number of roaches caught in traps in apartment $i$, with $i=1,\ldots, n = 262$. 

```{r roaches}
# Source: http://mc-stan.org/rstanarm/articles/count.html
data(roaches) 

# Roaches:
y = roaches$y

# Function to plot the point mass function:
stickplot = function(y, ...){
  js = 0:max(y); 
  plot(js, 
       sapply(js, function(js) mean(js == y)), 
       type='h', lwd=2, ...)
}
stickplot(y, main = 'PMF: Roaches Data',
          xlab = 'Roaches', ylab = 'Probability mass')
```

There are several notable features in the data:

1. Zero-inflation: `r round(100*mean(y==0), 0)`\% of the observations are zeros. 
2. (Right-) Skewness, which is clear from the histogram and common for (zero-inflated) count data.
3. Overdispersion: the sample mean is `r round(mean(y),0)` and the sample variance is `r round(var(y),0)`. 

A pest management treatment was applied to a subset of 158 apartments, with the remaining 104 apartments receiving a control. Additional data are available on the pre-treatment number of roaches, whether the apartment building is restricted to elderly residents, and the number of days for which the traps were exposed. We are interested in modeling how the roach incidence varies with these predictors.

There are two main modeling frameworks for this kind of data:

1. *Poisson and extensions*: the Poisson distribution is a valid count-valued distribution, and may incorporate predictions within a generalized linear modeling (GLM) framework. However, the Poisson distribution has limited modeling flexibility, which restricts practical utility in many applications. Extensions such as the zero-inflated Poisson, the negative-binomial, and the Conway-Maxwell distribuiton seek to enhance flexibility by adding parameters. While successful in some cases, these approaches require estimation of each additional parameter, which presents challenges especially in the presence of predictors or other (temporal or spatial) dependence.


2. *Transformed Gaussian*: in applied settings, it is perhaps more common to simply ignore the discrete nature of the data and apply continuous data models. For example, one might consider the transformed regression model

$$
z_i = \log(y_i + 1), \quad
y_i = x_i'\beta + \epsilon_i, \quad \epsilon_i \stackrel{iid}{\sim}N(0, \sigma^2)
$$
The transformation helps model the skewness often found in count data, while the regression model incorporates a simple and well-known continuous data model. However, this approach has two major flaws: (i) the transformation $z_i = \log(y_i + 1)$ requires the inclusion of an artificial constant to accommodate $y_i=0$, which is arbitrary and may introduce bias, and more importantly (ii) the implied data-generating process is *not* integer-valued, which induces a fundamental discrepancy between the data and the model. 


## Frequentist inference for STAR models
Frequentist (or classical) estimation and inference for STAR models is provided by an EM algorithm. Sufficient for estimation is an `estimator` function which solves the least squares (or Gaussian maximum likelihood) problem associated with $\mu_\theta$---or in other words, the estimator that *would* be used for Gaussian or continuous data. Specifically, `estimator` inputs data and outputs a list with two elements: the estimated `coefficients` $\hat \theta$ and the `fitted.values` $\hat \mu_\theta(x_i) = \mu_{\hat \theta}(x_i)$. The EM algorithm updates the parameters until convergence to the maximum likelihood estimators (MLEs), and is implemented in `star_EM`:

```{r freq-lm}
# Select a transformation:
transformation = 'log' # Log transformation
# transformation = 'np' # Estimated transformation using empirical CDF

# EM algorithm for STAR (using the log-link)
fit_em = lm_star(y ~ roach1 + treatment + senior + log(exposure2),
                 data = roaches, transformation = transformation)


# Dimensions:
n = nrow(fit_em$X); p = ncol(fit_em$X)

# Fitted coefficients:
round(coef(fit_em), 3)
```

Here the `log` transformation was used, but other options are available; see `?lm_star` for details. 

Based on the fitted STAR model, we may further obtain *confidence intervals* for the estimated coefficients using `confint`:

```{r conf}
# Confidence interval for the j=2 column:
j = 2
ci_j = confint(fit_em, level = 0.95,
        j = j,
        include_plot = FALSE)
print(round(ci_j, 3))

# Confidence for all columns:
ci_all = sapply(1:p, function(j)
  confint(fit_em, level = 0.95,
        j = j,
        include_plot = FALSE))
colnames(ci_all) = colnames(fit_em$X); 
rownames(ci_all) = c('Lower', 'Upper')
print(t(round(ci_all, 3)))

```

Similarly, *p-values* are available using likelihood ratio tests, which can be applied for individual coefficients,

$$
H_0: \beta_j= 0 \quad \mbox{vs} \quad H_1: \beta_j \ne 0
$$

or for joint sets of variables, analogous to a (partial) F-test:

$$
H_0: \beta_1=\ldots=\beta_p = 0, \quad \mbox{vs.} \quad H_1: \beta_j \ne 0 \mbox{ for some } j=1,\ldots,p
$$
```{r pval-all}
# p-value for *any* effects
# Note that the null model estimator *does not* include any X:
# P-values:
print(pvals(fit_em))
```

## Bayesian inference for STAR models

For a Bayesian model, STAR requires only an algorithm for *initializing and sampling* from the posterior distribution under a *continuous data model*. More specifically, posterior inference under STAR is based on a Gibbs sampler, which augments the aforementioned continuous sampler with a draw from $[z^* | y, \theta]$. When $\Pi_\theta$ is conditionally Gaussian, $[z^* | y, \theta]$ is a truncated Gaussian distribution. 

As an illustration, consider the Bayesian linear regression model
\begin{align*}
z_i^* &= x_i'\beta + \epsilon_i, \quad \epsilon_i \stackrel{iid}{\sim}N(0, \sigma^2)\\
\beta_j & \stackrel{iid}{\sim}N(0, \sigma_\beta^2), \quad \sigma_\beta \sim \mbox{Uniform}(0, 10^4)
\end{align*}
With STAR and a log transformation, posterior samples for $(\beta, \sigma_\beta, \sigma)$ are obtained using `star_MCMC` as follows:

```{r bayes-lm}
X = model.matrix(y ~ roach1 + treatment + senior + log(exposure2),
                 data = roaches)

# Dimensions:
n = nrow(X); p = ncol(X)

fit_mcmc = star_MCMC(y = y,
                          sample_params = function(y, params)
                            sample_params_lm(y, X, params),
                         init_params = function(y) 
                           init_params_lm(y,X),
                         transformation = 'log', verbose = FALSE)
```

The function `sample_params` computes a single draw of the parameters `params` conditional on continuous data. Here, the update is for Bayesian linear Gaussian regression, which samples from the posterior of $(\beta, \sigma_\beta, \sigma)$ conditional on the continuous latent data $z^*$. The function `init_params` simply initializes the parameters `params`. Posterior expectations and posterior credible intervals are available as follows:

```{r estimates-bayes}
# Posterior mean of each coefficient:
round(coef(fit_mcmc),3)

# Credible intervals for regression coefficients
ci_all_bayes = apply(fit_mcmc$post.coefficients[,1:p],
      2, function(x) quantile(x, c(.025, .975)))

# Rename and print:
colnames(ci_all_bayes) = colnames(X); rownames(ci_all_bayes) = c('Lower', 'Upper')
print(t(round(ci_all_bayes, 3)))
```

We may further evaluate the model based on posterior diagnostics and posterior predictive checks on the simulated versus observed proportion of zeros:

```{r diag}
# Posterior draws of the regression coefficients:
post.coef = fit_mcmc$post.coefficients[,2:p]
colnames(post.coef) = colnames(X)[2:p]

# MCMC diagnostics:
plot(as.ts(post.coef), main = 'Trace plots', cex.lab = .75)

# (Summary of) effective sample sizes across coefficients:
getEffSize(post.coef)

# Posterior predictive check:
hist(apply(fit_mcmc$post.pred, 1,
           function(x) mean(x==0)), main = 'Proportion of Zeros', xlab='');
abline(v = mean(y==0), lwd=4, col ='blue')
```


## Additional features in `rSTAR` 

* A fixed upper bound, `y_max`, with $y(x) \le \mbox{}$ `y_max` for all $x$ 

* Residual diagnostics in `star_EM`

* Log-likelihood at MLEs in `star_EM` for model comparison and information criteria (AIC or BIC)

* Customized functions for STAR with gradient boosting `gbm_star` and STAR with random forests `randomForest_star`

* Fitted values $\hat y(x) = E\{y(x)\}$ and posterior samples from $[\hat y(x) | y]$ in `star_MCMC`

* Samples from the *integer-valued* posterior predictive distribution $[\tilde y(x) | y]$ in `star_MCMC`, where $\tilde y$ denotes future or unobserved data

* Customized samplers for Bayesian additive models `sample_params_additive` and linear models with horseshoe priors `sample_params_lm_hs` 

* WAIC and pointwise log-likelihoods for (Bayesian) model comparisons

* A customized STAR model for Bayesian additive regression trees (BART) `bart_star_MCMC`

* Posterior samplers for STAR with unknown and nonparametric transformation $g$

* Monte Carlo samplers for posterior and predictive inference with linear regression and spline regression
