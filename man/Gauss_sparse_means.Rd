% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/source_MCMC.R
\name{Gauss_sparse_means}
\alias{Gauss_sparse_means}
\title{Stochastic search for the sparse normal means model}
\usage{
Gauss_sparse_means(
  y,
  psi = NULL,
  a_pi = 1,
  b_pi = 1,
  nsave = 1000,
  nburn = 1000,
  nskip = 0,
  verbose = TRUE
)
}
\arguments{
\item{y}{\code{n x 1} data vector}

\item{psi}{prior variance for the slab component;
if NULL, assume a Unif(0, n) prior}

\item{a_pi}{prior shape1 parameter for the inclusion probability;
default is 1 for uniform}

\item{b_pi}{prior shape2 parameter for the inclusion probability;
#' default is 1 for uniform}

\item{nsave}{number of MCMC iterations to save}

\item{nburn}{number of MCMC iterations to discard}

\item{nskip}{number of MCMC iterations to skip between saving iterations,
i.e., save every (nskip + 1)th draw}

\item{verbose}{logical; if TRUE, print time remaining}
}
\value{
a list with the following elements:
\itemize{
\item \code{post_gamma}: \code{nsave x n} samples from the posterior distribution
of the inclusion indicators
\item \code{post_pi}: \code{nsave} samples from the posterior distribution
of the inclusion probability
\item \code{post_psi}: \code{nsave} samples from the posterior distribution
of the prior precision
\item \code{post_theta}: \code{nsave} samples from the posterior distribution
of the regression coefficients
\item \code{sigma_hat}: estimate of the latent data standard deviation
}
}
\description{
Compute Gibbs samples from the posterior distribution
of the inclusion indicators for the sparse normal means model.
The inclusion probability is assigned a Beta(a_pi, b_pi) prior
and is learned as well.
}
\details{
We assume sparse normal means model of the form
y_i = theta_i + epsilon_i with a spike-and-slab prior on
theta_i.

There are several options for the prior variance \code{psi}.
First, it can be specified directly. Second, it can be assigned
a Uniform(0,n) prior and sampled within the MCMC
conditional on the sampled regression coefficients.
}
\examples{
# Simulate some data:
y = rnorm(n = 200)

# Fit the model:
fit = Gauss_sparse_means(y, nsave = 100, nburn = 100) # for a quick example
names(fit)

# Posterior inclusion probabilities:
pip = colMeans(fit$post_gamma)
plot(pip, y)

# Check the MCMC efficiency:
getEffSize(fit$post_theta) # coefficients

}
