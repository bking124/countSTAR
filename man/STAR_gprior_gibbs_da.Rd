% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/source_MCMC.R
\name{STAR_gprior_gibbs_da}
\alias{STAR_gprior_gibbs_da}
\title{Gibbs sampler (data augmentation) for STAR linear regression with a g-prior}
\usage{
STAR_gprior_gibbs_da(
  y,
  X,
  X_test = X,
  transformation = "np",
  y_max = Inf,
  psi = length(y),
  approx_Fz = FALSE,
  approx_Fy = FALSE,
  nsave = 1000,
  nburn = 1000,
  nskip = 0,
  verbose = TRUE
)
}
\arguments{
\item{y}{\code{n x 1} vector of observed counts}

\item{X}{\code{n x p} matrix of predictors}

\item{X_test}{\code{n0 x p} matrix of predictors for test data;
default is the observed covariates \code{X}}

\item{transformation}{transformation to use for the latent data; must be one of
\itemize{
\item "identity" (identity transformation)
\item "log" (log transformation)
\item "sqrt" (square root transformation)
\item "bnp" (Bayesian nonparametric transformation using the Bayesian bootstrap)
\item "np" (nonparametric transformation estimated from empirical CDF)
\item "pois" (transformation for moment-matched marginal Poisson CDF)
\item "neg-bin" (transformation for moment-matched marginal Negative Binomial CDF)
}}

\item{y_max}{a fixed and known upper bound for all observations; default is \code{Inf}}

\item{psi}{prior variance (g-prior)}

\item{approx_Fz}{logical; in BNP transformation, apply a (fast and stable)
normal approximation for the marginal CDF of the latent data}

\item{approx_Fy}{logical; in BNP transformation, approximate
the marginal CDF of \code{y} using the empirical CDF}

\item{nsave}{number of MCMC iterations to save}

\item{nburn}{number of MCMC iterations to discard}

\item{nskip}{number of MCMC iterations to skip between saving iterations,
i.e., save every (nskip + 1)th draw}

\item{verbose}{logical; if TRUE, print time remaining}
}
\value{
a list with the following elements:
\itemize{
\item \code{coefficients} the posterior mean of the regression coefficients
\item \code{post_beta}: \code{nsave x p} samples from the posterior distribution
of the regression coefficients
\item \code{post_ytilde}: \code{nsave x n0} samples
from the posterior predictive distribution at test points \code{X_test}
\item \code{post_g}: \code{nsave} posterior samples of the transformation
evaluated at the unique \code{y} values (only applies for 'bnp' transformations)
}
}
\description{
Compute MCMC samples from the posterior and predictive
distributions of a STAR linear regression model with a g-prior.
The Monte Carlo sampler \code{STAR_gprior} is preferred unless \code{n}
is large (> 500).
}
\details{
STAR defines a count-valued probability model by
(1) specifying a Gaussian model for continuous *latent* data and
(2) connecting the latent data to the observed data via a
*transformation and rounding* operation. Here, the continuous
latent data model is a linear regression.

There are several options for the transformation. First, the transformation
can belong to the *Box-Cox* family, which includes the known transformations
'identity', 'log', and 'sqrt'. Second, the transformation
can be estimated (before model fitting) using the empirical distribution of the
data \code{y}. Options in this case include the empirical cumulative
distribution function (CDF), which is fully nonparametric ('np'), or the parametric
alternatives based on Poisson ('pois') or Negative-Binomial ('neg-bin')
distributions. For the parametric distributions, the parameters of the distribution
are estimated using moments (means and variances) of \code{y}. The distribution-based
transformations approximately preserve the mean and variance of the count data \code{y}
on the latent data scale, which lends interpretability to the model parameters.
Lastly, the transformation can be modeled using the Bayesian bootstrap ('bnp'),
which is a Bayesian nonparametric model and incorporates the uncertainty
about the transformation into posterior and predictive inference.
}
\examples{
# Simulate some data:
sim_dat = simulate_nb_lm(n = 500, p = 10)
y = sim_dat$y; X = sim_dat$X

# Fit a linear model:
fit = STAR_gprior_gibbs_da(y, X,
                          transformation = 'np',
                          nsave = 1000, nburn = 1000)
names(fit)

# Check the efficiency of the MCMC samples:
getEffSize(fit$post_beta)

}
