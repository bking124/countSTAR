% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/source_EM.R
\name{gbm_star}
\alias{gbm_star}
\title{EM Algorithm for STAR Gradient Boosting Machines}
\usage{
gbm_star(y, X, X.test = NULL, n.trees = 100, interaction.depth = 1,
  shrinkage = 0.1, bag.fraction = 1, transformation = "log",
  lambda = NULL, y_max = Inf, tol = 10^-6, max_iters = 500)
}
\arguments{
\item{y}{\code{n x 1} vector of observed counts}

\item{X}{\code{n x p} matrix of predictors}

\item{X.test}{\code{m x p} matrix of out-of-sample predictors}

\item{n.trees}{Integer specifying the total number of trees to fit.
This is equivalent to the number of iterations and the number of basis functions in the additive expansion.
Default is 100.}

\item{interaction.depth}{Integer specifying the maximum depth of each tree
(i.e., the highest level of variable interactions allowed).
A value of 1 implies an additive model, a value of 2 implies a model with up to 2-way interactions, etc.
Default is 1.}

\item{shrinkage}{a shrinkage parameter applied to each tree in the expansion.
Also known as the learning rate or step-size reduction; 0.001 to 0.1 usually work, but a smaller learning rate typically requires more trees.
Default is 0.1.}

\item{bag.fraction}{the fraction of the training set observations randomly selected to propose the next tree in the expansion.
This introduces randomnesses into the model fit. If bag.fraction < 1 then running the same model twice will result in similar but different fits.
Default is 1 (for a deterministic prediction).}

\item{transformation}{transformation to use for the latent process; must be one of
\itemize{
\item "identity" (identity transformation)
\item "log" (log transformation)
\item "sqrt" (square root transformation)
\item "box-cox" (box-cox transformation)
}}

\item{lambda}{the nonlinear parameter for the Box-Cox transformation; otherwise ignored}

\item{y_max}{a fixed and known upper bound for all observations; default is \code{Inf}}

\item{tol}{tolerance for stopping the EM algorithm; default is 10^-10;}

\item{max_iters}{maximum number of EM iterations before stopping; default is 1000}
}
\value{
a list with the following elements:
\itemize{
\item \code{fitted.values}: the fitted values at the MLEs (training)
\item \code{fitted.values.test}: the fitted values at the MLEs (testing)
\item \code{sigma.hat}: the MLE of the standard deviation
\item \code{mu.hat}: the MLE of the conditional mean (on the transformed scale)
\item \code{z.hat}: the estimated latent variables (on the transformed scale) at the MLEs
\item \code{residuals}: the Dunn-Smyth residuals
\item \code{logLik}: the log-likelihood at the MLEs
\item \code{cond.pred.test}: 1000 simulated datasets at test points conditional on the MLEs
\item \code{gbmObj}: the object returned by gbm() at the MLEs
\item and other parameters that
(i) track the parameters across EM iterations and
(ii) record the model specifications
}
}
\description{
Compute the MLEs and log-likelihood for the Gradient Boosting Machines (GBM) STAR model.
The STAR model requires a transformation (such as log, sqrt, or Box-Cox)
and will estimate the conditional mean on the transformed scale using GBMs.
Standard function calls including \code{fitted()} and \code{residuals()} apply.
}
\note{
For the Box-Cox transformation, a \code{NULL} value of
\code{lambda} requires estimation of \code{lambda}. The maximum likelihood
estimator is computed over a grid of values within the EM algorithm.
}
\examples{
# Simulate data with count-valued response y:
sim_dat = simulate_nb_friedman(n = 100, p = 10)
y = sim_dat$y; X = sim_dat$X

# EM algorithm for STAR (using the log-link)
fit_em = gbm_star(y = y, X = X,
                 transformation = 'log')

# Evaluate convergence:
plot(fit_em$logLik_all, type='l', main = 'GBM-STAR-log', xlab = 'Iteration', ylab = 'log-lik')

# Fitted values:
y_hat = fitted(fit_em)
plot(y_hat, y);

# Residuals:
plot(residuals(fit_em))
qqnorm(residuals(fit_em)); qqline(residuals(fit_em))

# Log-likelihood at MLEs:
fit_em$logLik

}
